---
title: 'POP77022: Programming Exercise 1'
author: "Darragh Kane O Toole"
date: "Today's date"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

The first homework assignment will cover concepts and methods from Weeks 1 & 2 (basic string operations, corpus acquisition, text processing, textual statistics, dictionary methods).  You are expected to provide your answers as embedded R code and/or text answers in the chunks provided in the homework RMarkdown file. 

For example:

```{r}
print("Print R code in code chunk.")
```

```
Describe results and provide answers to conceptual and open-ended questions
in a plain code block like this one.
```

__The programming exercise is worth 20% of your total grade.  The questions sum to 100 points.__

## Analysis of tweets during a political crisis

We will start with a dataset that contains almost 900 tweets that were published by four central figures in American politics around the time of the onset of an impeachment inquiry: Pres. Donald Trump, Rudy Giuliani, Speaker of the House Rep. Nancy Pelosi, and Chair of the House Intelligence Committee Rep. Adam Schiff.  

The first step will be to read the spreadsheet of tweets into R and then use the `str` and `head` functions to describe the variables and contents of the dataset.  For your convenience, I will provide code to import the spreadsheet (*Hint: be sure that the data folder is in the same folder as this homework RMarkdown file.*)

```{r include=FALSE}
setwd(getwd())
data <- read.csv("~/Desktop/QTA_Spring23/homework1/us_tweets.csv", 
             stringsAsFactors=FALSE,
                  encoding = "utf-8")
us_tweets <- data
pkgTest <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg,  dependencies = TRUE)
  sapply(pkg,  require,  character.only = TRUE)
}

lapply(c("tidyverse",
         "guardianapi", # for working with the Guardian's API
         "quanteda", # for QTA
         "quanteda.textstats", # more Quanteda!
         "quanteda.textplots", # even more Quanteda!
         "readtext", # for reading in text data
         "stringi", # for working with character strings
         "textstem", # an alternative method for lemmatizing
         "lubridate" # working with dates
), pkgTest)
library('tidyverse')

```

### Question 1.0 (2 points)

Print the number of tweets that are in this dataset.

```{r}
# Insert code here
glimpse(us_tweets)
print(length(us_tweets$X))
```


### Question 1.1 (3 points)

Create a new dataframe that only includes original tweets (remove retweets) and print the number of rows.

```{r}
# Insert code here
originals <- us_tweets%>%
  filter(is_retweet == "FALSE")
summary(originals)
length(originals$X)
```

### Question 1.2 (20 points)

Create a smaller dataframe that only includes tweets by Donald Trump.

* Print how many tweets by Trump are contained in the dataset?

For the following print the number of instances as well as an example tweet:

* How many tweets include an exclamation mark?  
* In how many tweets did Trump mention words related to "winning"?
* "employment"?
* "immigration"?
* "hoax"?

Make sure that you support your answers with code.

(*Hints: be sure to use regular expressions when searching the tweets; also you might want to wrap your search term in between word anchor boundaries (`\\b`).  For instance, for the term health: `"\\bhealth\\b"`*)
```{r echo=TRUE}
trump <- us_tweets%>%
  filter(screen_name == "realDonaldTrump")
length(trump$X)
##corpus made
tcorp <- corpus(trump, 
                 docid_field = "X",
                 text_field = "text")

sumtcorp <- summary(tcorp, 
                     n = nrow(docvars(tcorp))) #note: the default is n=100
#tokenise
trumptok <- quanteda::tokens(tcorp, 
                             include_docvars = TRUE,
                             remove_numbers = TRUE,
                             remove_punct = FALSE,
                             remove_symbols = FALSE,
                             remove_separators = TRUE,
                             remove_url = TRUE) %>% #Create tokens object
  tokens_tolower() %>% # Transform to lower case
  tokens_remove(stopwords("english")) # Remove stopwords
##frequency
tokens_wordstem(trumptok)
t_dft <- dfm(trumptok)
```

```{r}
# Insert code here
trump <- us_tweets%>%
  filter(screen_name == "realDonaldTrump")
length(trump$X)
#600
ttok <- quanteda::tokens(tcorp, 
                             include_docvars = TRUE,
                             remove_numbers = TRUE,
                             remove_punct = TRUE,
                             remove_symbols = TRUE,
                             remove_separators = TRUE,
                             remove_url = TRUE) %>% #Create tokens object
  tokens_tolower() %>% # Transform to lower case
  tokens_remove(stopwords("english")) # Remove stopwords
tokens_wordstem(ttok)
t_dft <- dfm(ttok)
tfreq <- textstat_frequency(t_dft, n = 30)
##frequency of "!"
tfreq%>% 
  filter(feature== "!")

#frequency of "win
tfreq%>% 
  filter(feature == "win")

#freqeuncy of employment 
tfreq%>% 
  filter(feature == "employment")

#frequency of immigration
tfreq%>% 
  filter(feature == "immigration")
#6 hoax
tfreq%>% 
  filter(feature == "hoax")

```


### Question 2 (75 points)

Create a `corpus` and a `dfm` object with processed text (including collocations) using the dataframe generated in Question 1.1.  With the generated `dfm` object perform the following tasks:

1. Create a frequency plot of the top 30 tokens for each politician.
1. Determine the "key" terms that Trump and Pelosi are more likely to tweet.  Plot your results
1. Perform a keyword in context analysis using your `corpus` object for some of the most distinct keywords from both Trump and Pelosi. *Hint: remember to use the `phrase` function in the `pattern` argument of `kwic`*
1. Conduct a sentiment analysis of Trump's tweets using the Lexicon Sentiment Dictionary.  Plot net sentiment over the entire sample period. Interpret the results.  *Hint: you might want to use `lubridate` to generate a date object variable from the "created_at" variable before plotting.  For example: `docvars(dfm, "date") <- lubridate::ymd_hms(dfm@docvars$created_at)` *
1. Justify each of your text processing decisions and interpret your results in the text field below. What can we learn about the political communication surrounding the political crisis based on the results from the above tasks?

```{r echo=TRUE}
library(quanteda)
###frequency for all
unique(us_tweets$screen_name)
###trump
ttok <- quanteda::tokens(tcorp, 
                             include_docvars = TRUE,
                             remove_numbers = TRUE,
                             remove_punct = TRUE,
                             remove_symbols = TRUE,
                             remove_separators = TRUE,
                             remove_url = TRUE) %>% #Create tokens object
  tokens_tolower() %>% # Transform to lower case
  tokens_remove(stopwords("english")) # Remove stopwords
tokens_wordstem(ttok)
t_dft <- dfm(ttok)
tfreq <- textstat_frequency(t_dft, n = 30)
###rudy
rudy <- us_tweets%>%
  filter(screen_name == "RudyGiuliani")


rcorp <- corpus(rudy, 
                docid_field = "X",
                text_field = "text")

sumrcorp <- summary(rcorp, 
                    n = nrow(docvars(rcorp))) #note: the default is n=100
#tokenise
rtok <- quanteda::tokens(rcorp, 
                             include_docvars = TRUE,
                             remove_numbers = TRUE,
                             remove_punct = TRUE,
                             remove_symbols = TRUE,
                             remove_separators = TRUE,
                             remove_url = TRUE) %>% #Create tokens object
  tokens_tolower() %>% # Transform to lower case
  tokens_remove(stopwords("english")) # Remove stopwords
##frequency
tokens_wordstem(rtok)
r_dft <- dfm(rtok)
rfreq <- textstat_frequency(r_dft, n = 30)
#############pelosi
pelosi <- us_tweets%>%
  filter(screen_name == "SpeakerPelosi")
##corpus 
pcorp <- corpus(pelosi, 
                docid_field = "X",
                text_field = "text")

sumpcorp <- summary(pcorp, 
                    n = nrow(docvars(pcorp))) #note: the default is n=100
#tokenise
ptok <- quanteda::tokens(pcorp, 
                         include_docvars = TRUE,
                         remove_numbers = TRUE,
                         remove_punct = TRUE,
                         remove_symbols = TRUE,
                         remove_separators = TRUE,
                         remove_url = TRUE) %>% #Create tokens object
  tokens_tolower() %>% # Transform to lower case
  tokens_remove(stopwords("english")) # Remove stopwords
##frequency
tokens_wordstem(ptok)
p_dft <- dfm(ptok)
pfreq <- textstat_frequency(p_dft, n = 30)

#####schiff
schiff <- us_tweets%>%
  filter(screen_name == "RepAdamSchiff")

scorp <- corpus(schiff, 
                docid_field = "X",
                text_field = "text")

sumscorp <- summary(scorp, 
                    n = nrow(docvars(scorp))) #note: the default is n=100
#tokenise
stok <- quanteda::tokens(scorp, 
                         include_docvars = TRUE,
                         remove_numbers = TRUE,
                         remove_punct = TRUE,
                         remove_symbols = TRUE,
                         remove_separators = TRUE,
                         remove_url = TRUE) %>% #Create tokens object
  tokens_tolower() %>% # Transform to lower case
  tokens_remove(stopwords("english")) # Remove stopwords
##frequency
tokens_wordstem(stok)
s_dft <- dfm(stok)
sfreq <- textstat_frequency(s_dft, n = 30)
# Insert code here

```

```{r echo=FALSE}
##2.1 code hidden for conveniance
#trump
print(tfreq) 
#rudy
print(rfreq)
#pelosi
print(pfreq)
#schiff
print(sfreq)


```
```{r echo=TRUE}
tp <- us_tweets%>%
  filter(screen_name == "SpeakerPelosi" | screen_name =="realDonaldTrump")



tp_corp <- corpus(tp$text, 
                docid_field = "text",
                text_field = "screen_name")

sumtpcorp <- summary(tp_corp, 
                    n = nrow(docvars(tp_corp))) #note: the default is n=100
#tokenise
tp_tok <- quanteda::tokens(tp_corp, 
                         include_docvars = TRUE,
                         remove_numbers = TRUE,
                         remove_punct = TRUE,
                         remove_symbols = TRUE,
                         remove_separators = TRUE,
                         remove_url = TRUE) %>% #Create tokens object
  tokens_tolower() %>% # Transform to lower case
  tokens_remove(stopwords("english")) # Remove stopwords
##frequency
tokens_wordstem(tp_tok)
tp_dft <- dfm(tp_tok)
tpfreq <- textstat_frequency(tp_dft, n = 30)
##now with collocations
colc_tp <- textstat_collocations(tp_tok, size = 2, min_count = 8)
```

```{r}
###2.2 Trump and pelosi 
print(colc_tp)
```

```{r}
##2.3 
trump_kwic <- kwic(trumptok,
                     pattern = phrase(tfreq$feature),
                     window = 3,
                     case_insensitive = TRUE)
print(trump_kwic)

pelosi_kwic <- kwic(ptok,
                   pattern = phrase(pfreq$feature),
                   window = 3,
                   case_insensitive = TRUE)
print(pelosi_kwic)
```

```{r echo=TRUE}
dfmtrump <- dfm(ttok)
docvars(dfmtrump, "date") <- lubridate::ymd_hms(dfmtrump@docvars$created_at)


dfm_sentiment <- dfm_lookup(dfmtrump, data_dictionary_LSD2015[1:2]) %>%
  dfm_group(groups = date)

# Once we have the frequency for positive and negative sentiment, we can 
# use a useful feature of R - vectorisation - to calculate net sentiment 
# across each day and plot it.
docvars(dfm_sentiment, "prop_negative") <- as.numeric(dfm_sentiment[,1] / ntoken(dfm_sentiment))
docvars(dfm_sentiment, "prop_positive") <- as.numeric(dfm_sentiment[,2] / ntoken(dfm_sentiment))
docvars(dfm_sentiment, "net_sentiment") <- docvars(dfm_sentiment, "prop_positive") - docvars(dfm_sentiment,"prop_negative")

docvars(dfm_sentiment) %>%
  ggplot(aes(x = yday(date), y = net_sentiment, group = year(date))) +
  geom_smooth(aes(colour = as.character(year(date)))) +
  labs(title = "Trump sentiment over time", 
       x = "day of year", y = "net sentiment", 
       colour = "year")

```

```
<Insert open-ended response here>
The mehtodology I have used was the simplest way I could do it, with decisions 
some deciosion being made due to review ing of data outputs such as the kwic and 
key terms which I tried using a variety of lengths and preferences. 

The text ananlysed accross all leaders shows a tendancy to for leaders to boast about themselfs and to bash their opposition. Examples of this are trumps most used words including president,his username and trump. This is also seen in the language of other leaders to a lesser extent with names of other political leader being common in the tweets.

The other 2 questions add to this through context showing alot of language thta is linked to blame and complain. 

Then this concludes with the trump sentiment which is shwon to be get progressively more negative as the blam retoric continues. 
```




